{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxlHSW3RQ3I0",
        "outputId": "0ed22dcb-03a6-425d-9ccb-1b21e1818f0d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ],
      "metadata": {
        "id": "ITwhpy2RQv48"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "BN96RnxqYOiY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your pandas DataFrame\n",
        "data = pd.read_csv(\"/content/sentiment_data.csv\")  # Replace with your actual file path\n",
        "\n",
        "# Define labels\n",
        "labels = data['Sentiment'].apply(lambda x: 0 if x == 0.0 else (1 if x == 0.5 else 2))\n",
        "#print(labels)\n",
        "\n",
        "# Tokenize and encode the sentences\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "max_length = 80  # You can adjust this based on your specific needs\n",
        "\n",
        "# Tokenize and encode sentences\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "# Define a scaler for gradient scaling\n",
        "scaler = GradScaler()\n",
        "\n",
        "for sentence in data['Sentence']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels.values)\n"
      ],
      "metadata": {
        "id": "G0w6iPV2TtPu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = train_test_split(\n",
        "    input_ids, labels, attention_masks, random_state=42, test_size=0.1)\n",
        "\n",
        "# Further split the test set into validation and test sets\n",
        "val_inputs, test_inputs, val_labels, test_labels, val_masks, test_masks = train_test_split(\n",
        "    test_inputs, test_labels, test_masks, random_state=42, test_size=0.5)\n"
      ],
      "metadata": {
        "id": "xyz-m1fYTtSN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create DataLoader for training data\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create DataLoader for validation data\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create DataLoader for test data\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "Z7mOgKPPTtUf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=3,  # Three classes: positive, negative, neutral\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "# Set up GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD7_d6HJTtYZ",
        "outputId": "b5caf5d1-6955-4960-fd24-d732cc259f0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
        "\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(f\"Validation Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n",
        "\n",
        "# Classification report on the validation set\n",
        "predicted_labels = np.argmax(np.concatenate(predictions, axis=0), axis=1)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "report = classification_report(true_labels, predicted_labels, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "hVm45bgWU1rA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53775acf-3f55-4d6b-b87d-f06a9702ed15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average Training Loss: 0.6528\n",
            "Epoch 2 - Average Training Loss: 0.3684\n",
            "Epoch 3 - Average Training Loss: 0.2640\n",
            "Epoch 4 - Average Training Loss: 0.2067\n",
            "Epoch 5 - Average Training Loss: 0.1711\n",
            "Epoch 6 - Average Training Loss: 0.1540\n",
            "Epoch 7 - Average Training Loss: 0.1464\n",
            "Epoch 8 - Average Training Loss: 0.1366\n",
            "Epoch 9 - Average Training Loss: 0.1369\n",
            "Epoch 10 - Average Training Loss: 0.1305\n",
            "Validation Accuracy: 0.7625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.40      0.45      0.42        40\n",
            "     neutral       0.84      0.78      0.81       163\n",
            "    positive       0.82      0.88      0.85        89\n",
            "\n",
            "    accuracy                           0.76       292\n",
            "   macro avg       0.69      0.70      0.69       292\n",
            "weighted avg       0.77      0.76      0.77       292\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
        "\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(f\"Test Accuracy: {eval_accuracy / nb_eval_steps:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbY80m8QTtbF",
        "outputId": "ec83587b-1880-40b8-f3db-3a53ac69acc6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report on the test set\n",
        "predicted_labels = np.argmax(np.concatenate(predictions, axis=0), axis=1)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "report = classification_report(true_labels, predicted_labels, target_names=[\"negative\", \"neutral\", \"positive\"])\n",
        "\n",
        "print(report)\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(\"bert_sentiment_model\")\n",
        "\n",
        "# Optionally, you can load the model later using:\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert_sentiment_model\")\n",
        "\n",
        "# You can also use the model for inference on new data\n",
        "# For example:\n",
        "# new_sentence = \"This is a positive sentence.\"\n",
        "# encoded_dict = tokenizer.encode_plus(\n",
        "#     new_sentence,\n",
        "#     add_special_tokens=True,\n",
        "#     max_length=max_length,\n",
        "#     padding='max_length',\n",
        "#     return_attention_mask=True,\n",
        "#     return_tensors='pt',\n",
        "#     truncation=True\n",
        "# )\n",
        "# input_ids = encoded_dict['input_ids'].to(device)\n",
        "# attention_mask = encoded_dict['attention_mask'].to(device)\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "# logits = outputs.logits\n",
        "# predicted_class = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "# print(f\"Predicted Class: {predicted_class}\")\n",
        "\n",
        "# You can adjust the model architecture, hyperparameters, and other settings as needed.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3QFteQ7T99J",
        "outputId": "875f63b2-1c97-401a-d912-662f81d30827"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.53      0.48      0.51        50\n",
            "     neutral       0.80      0.83      0.82       149\n",
            "    positive       0.88      0.87      0.88        94\n",
            "\n",
            "    accuracy                           0.78       293\n",
            "   macro avg       0.74      0.73      0.73       293\n",
            "weighted avg       0.78      0.78      0.78       293\n",
            "\n"
          ]
        }
      ]
    }
  ]
}